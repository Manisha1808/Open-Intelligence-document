# Open Document Intelligence System ğŸ“„

A **Retrieval-Augmented Generation (RAG)** based document intelligence system that can read PDFs, understand their content, and answer user questions using **semantic search + Large Language Models**.

This project is designed as a **long-term, scalable GenAI system**, not just a demo.

---

## ğŸš€ Features (Current)

- ğŸ“„ **PDF Ingestion**
  - Automatically loads all PDFs from a documents directory
  - Supports multiple real-world documents

- âœ‚ï¸ **Smart Chunking**
  - Breaks documents into manageable semantic chunks

- ğŸ”¢ **Embeddings + Vector Search**
  - Uses HuggingFace embeddings
  - Stores vectors in **FAISS** for fast semantic retrieval
  - FAISS index is **persisted to disk** (no recomputation on restart)

- ğŸ§  **Retrieval-Augmented Generation (RAG)**
  - Retrieves relevant chunks
  - Passes grounded context to the LLM
  - Prevents hallucination

- ğŸ”€ **Intent Routing**
  - Distinguishes between:
    - Document-level questions (summary/overview)
    - Fact-based questions (RAG)
  - Uses semantic similarity instead of keywords

- ğŸ“ **Document-Level Summaries**
  - Generates and stores summaries per document
  - Summaries are reused for overview-type questions

- ğŸŒ **FastAPI Backend**
  - Exposes the RAG pipeline as an API
  - Ready for UI and frontend integration
    


Open-Intelligence-document/
â”‚
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ loaders.py # PDF loading
â”‚ â”œâ”€â”€ chunking.py # Text chunking
â”‚ â”œâ”€â”€ embeddings.py # Embedding + FAISS logic
â”‚ â”œâ”€â”€ retriever.py # Vector retrieval
â”‚ â”œâ”€â”€ rag_chain.py # RAG pipeline
â”‚ â”œâ”€â”€ intent_router.py # Summary vs RAG routing
â”‚ â”œâ”€â”€ summarizer.py # Document summarization
â”‚ â”œâ”€â”€ summary_store.py # Persist summaries
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ documents/ # PDF documents
â”‚ â””â”€â”€ summaries.json # Stored summaries
â”‚
â”œâ”€â”€ vector_store/
â”‚ â”œâ”€â”€ index.faiss # FAISS vectors
â”‚ â””â”€â”€ index.pkl # FAISS metadata
â”‚
â”œâ”€â”€ api/
â”‚ â””â”€â”€ main.py # FastAPI entrypoint
â”‚
â”œâ”€â”€ main.py # CLI entrypoint
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â””â”€â”€ .gitignore
---


---

## â–¶ï¸ How It Works (High-Level)

1. PDFs are loaded and chunked  
2. Chunks are converted into embeddings  
3. Embeddings are stored in FAISS  
4. User question is analyzed:
   - **Overview question** â†’ document summary
   - **Factual question** â†’ RAG pipeline
5. LLM generates a grounded answer using retrieved context  

---

## ğŸ§ª Run Locally

### 1ï¸âƒ£ Install dependencies
```bash
uv sync

2ï¸âƒ£ Activate environment
.venv\Scripts\activate   # Windows

3ï¸âƒ£ Run CLI version
python main.py

4ï¸âƒ£ Run API
uvicorn api.main:app --reload


Visit:

http://127.0.0.1:8000/docs

ğŸ” Security

Secrets are managed via environment variables

.env is excluded using .gitignore

No sensitive keys are committed to GitHub

ğŸ›£ï¸ Roadmap (Planned)

Incremental indexing for new documents

Source attribution in answers

Confidence-based fallback responses

UI integration

Deployment

ğŸ¤ Interview Summary

â€œI built a RAG-based document intelligence system using LangChain, FAISS, and FastAPI.
It supports semantic retrieval, document-level summaries, persistent vector storage, and API-based access.â€

ğŸ“Œ Tech Stack

Python

LangChain

FAISS

HuggingFace Embeddings

Ollama (Local LLM)

FastAPI

ğŸ§  Author

Manisha Sen
BE Computer Engineering
Focused on Data & GenAI Systems

## ğŸ—ï¸ Project Structure

